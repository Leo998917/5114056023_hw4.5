import requests
from bs4 import BeautifulSoup
import csv
import time
import random

# è¨­å®šç›®æ¨™ç¶²ç«™åŸºç¤ç¶²å€
BASE_URL = "https://ssr1.scrape.center/page/{}"
# è¨­å®š CSV æª”æ¡ˆåç¨±
CSV_FILENAME = "movie.csv"

def fetch_page(page_number):
    """
    çˆ¬å–å–®ä¸€é é¢çš„ HTML
    """
    url = BASE_URL.format(page_number)
    print(f"ğŸ“¥ æ­£åœ¨çˆ¬å–ç¬¬ {page_number} é : {url}")
    
    try:
        # ç™¼é€ GET è«‹æ±‚
        # verify=False æ˜¯ç‚ºäº†é˜²æ­¢æŸäº›ç’°å¢ƒä¸‹çš„ SSL éŒ¯èª¤ (è·Ÿ Part 1 ä¸€æ¨£)
        response = requests.get(url, verify=False) 
        if response.status_code == 200:
            return response.text
        else:
            print(f"âŒ ç¬¬ {page_number} é çˆ¬å–å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
            return None
    except Exception as e:
        print(f"âŒ é€£ç·šç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def parse_html(html):
    """
    ä½¿ç”¨ BeautifulSoup è§£æ HTML ä¸¦æå–é›»å½±è³‡è¨Š
    """
    soup = BeautifulSoup(html, "html.parser")
    movies = []
    
    # å°‹æ‰¾æ‰€æœ‰é›»å½±å¡ç‰‡ (æ ¹æ“šç¶²ç«™çµæ§‹ï¼Œé€šå¸¸æ˜¯ .el-card__body æˆ– .item)
    # è§€å¯Ÿ ssr1.scrape.centerï¼Œæ¯å€‹é›»å½±éƒ½åœ¨ä¸€å€‹ class="el-card item m-t is-hover-shadow" è£¡
    items = soup.find_all(class_="el-card")
    
    for item in items:
        try:
            # 1. é›»å½±åç¨± (é€šå¸¸åœ¨ h2 æ¨™ç±¤)
            title_tag = item.find("h2")
            title = title_tag.text.strip() if title_tag else "N/A"
            
            # 2. åœ–ç‰‡ URL (img æ¨™ç±¤çš„ src)
            img_tag = item.find("img", class_="cover")
            cover_url = img_tag["src"] if img_tag else "N/A"
            
            # 3. è©•åˆ† (class="score")
            score_tag = item.find(class_="score")
            score = score_tag.text.strip() if score_tag else "N/A"
            
            # 4. é¡å‹ (class="categories") -> è£¡é¢æœ‰å¤šå€‹ button
            categories_tag = item.find(class_="categories")
            if categories_tag:
                # æ‰¾å‡ºè£¡é¢æ‰€æœ‰æŒ‰éˆ•æ–‡å­—ï¼Œåˆä½µæˆå­—ä¸²
                cats = [btn.text.strip() for btn in categories_tag.find_all("button")]
                category = ", ".join(cats) # ä¾‹å¦‚: "åŠ‡æƒ…, æ„›æƒ…"
            else:
                category = "N/A"
            
            # æ•´ç†æˆå­—å…¸
            movie_data = {
                "Title": title,
                "Cover URL": cover_url,
                "Score": score,
                "Category": category
            }
            movies.append(movie_data)
            
        except Exception as e:
            print(f"âš ï¸ è§£æå–®ç­†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
            
    return movies

def save_to_csv(all_movies):
    """
    å°‡è³‡æ–™å¯«å…¥ CSV
    """
    if not all_movies:
        print("âš ï¸ æ²’æœ‰è³‡æ–™å¯ä»¥å¯«å…¥ CSV")
        return

    fieldnames = ["Title", "Cover URL", "Score", "Category"]
    
    try:
        with open(CSV_FILENAME, mode="w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader() # å¯«å…¥æ¨™é¡Œåˆ—
            writer.writerows(all_movies) # å¯«å…¥è³‡æ–™
        print(f"âœ… æˆåŠŸå„²å­˜ {len(all_movies)} ç­†é›»å½±è³‡æ–™åˆ° {CSV_FILENAME}ï¼")
    except Exception as e:
        print(f"âŒ å¯«å…¥ CSV å¤±æ•—: {e}")

def main():
    import urllib3
    urllib3.disable_warnings() # é—œé–‰ SSL è­¦å‘Š
    
    all_movies = []
    total_pages = 10 # é¡Œç›®è¦æ±‚çˆ¬ 1~10 é 
    
    print("ğŸš€ é›»å½±çˆ¬èŸ²å•Ÿå‹•...")
    
    for page in range(1, total_pages + 1):
        html = fetch_page(page)
        if html:
            page_movies = parse_html(html)
            all_movies.extend(page_movies)
            print(f"   ğŸ“„ ç¬¬ {page} é è§£æå®Œæˆï¼ŒæŠ“åˆ° {len(page_movies)} ç­†è³‡æ–™")
        
        # ç¦®è²Œæ€§æš«åœï¼Œé¿å…å°ä¼ºæœå™¨é€ æˆå¤ªå¤§è² æ“” (é›–ç„¶æ˜¯ç·´ç¿’ç«™ï¼Œä½†é€™æ˜¯å¥½ç¿’æ…£)
        time.sleep(random.uniform(0.5, 1.5))
        
    print("-" * 30)
    print(f"ğŸ“Š ç¸½å…±æŠ“å– {len(all_movies)} ç­†é›»å½±è³‡æ–™")
    save_to_csv(all_movies)

if __name__ == "__main__":
    main()